{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahynski/chemometric-carpentry/blob/main/notebooks/2_Techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "‚ùì ***Objective***: This notebook will introduce the basic techniques used when creating and optimizing predictive models.  Some steps will be reviewed in much more detail later.\n",
        "\n",
        "üîÅ ***Remember***: You can always revisit this notebook for reference again in the future.  Ideas and best practices will be reinforced in future notebooks, so don't worry about remembering everything the first time you see something new.\n",
        "\n",
        "üßë Author: Nathan A. Mahynski\n",
        "\n",
        "üìÜ Date: May 8, 2024\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "5jwEj64GdB3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "gaxOJlo5c_ot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üëÜ [EDA](https://en.wikipedia.org/wiki/Exploratory_data_analysis) is the first step in the modeling process.  \n",
        "\n",
        "The basic purpose of EDA is start to \"play\" with your data.  It is usually a very visual process which is meant to generate hypotheses and alert the modeler to (unexpected) trends. The term [\"statistical graphics\"](https://en.wikipedia.org/wiki/Statistical_graphics) is often used interchangeably with, [though it is different](https://www.itl.nist.gov/div898/handbook/eda/section1/eda11.htm). The term is attributed to John Tukey, who wrote the book entitled [\"Exploratory Data Analysis\"](https://en.wikipedia.org/wiki/Exploratory_data_analysis#cite_note-Tukey1977-6) in 1977.  \n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/b/ba/Data_visualization_process_v1.png\" align=\"right\" height=300 />\n",
        "\n",
        "üëâ EDA is a philosophy ü§î not a set of techniques! Nonetheless, EDA is usually done by making lots of different plots of the data to help the human observing the data to recognize patterns.\n",
        "\n",
        "[NIST has some great resources on EDA.](https://www.itl.nist.gov/div898/handbook/eda/eda.htm)  Here is a direct quote:\n",
        "\n",
        "> \"Exploratory Data Analysis (EDA) is an approach/philosophy for data analysis  that employs a variety of techniques (mostly graphical) to:\n",
        "* maximize insight into a data set\n",
        "* uncover underlying structure\n",
        "* extract important variables\n",
        "* detect outliers and anomalies\n",
        "* test underlying assumptions\n",
        "* develop parsimonious models and\n",
        "* determine optimal factor settings.\"\n",
        "\n",
        "EDA should not be confused with [\"initial data analysis\" (IDA)](https://en.wikipedia.org/wiki/Data_analysis#Initial_data_analysis) which is a more narrow term that encompasses data cleaning (handling missing values, data transformations, etc.); in fact, IDA is part of EDA.\n",
        "\n",
        "EDA can sometimes be controversial as there is a school of thought that suggests EDA can bias the data scientist at the outset toward false correlations or unsupported conclusions. While this can be true, for most scientific applications the injection of background training/intuition at this stage is often more beneficial than harmful. Still, you should always be wary of bias."
      ],
      "metadata": {
        "id": "9FyUKAPadKQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ü§î Remember that EDA is basically just the process of plotting your data in lots of different ways to gain insight.  There is not necessarily a \"right\" or \"wrong\" way to do it, but some approaches are more helpful than others.  [ITL@NIST](https://www.itl.nist.gov/div898/handbook/eda/eda.htm) has some good examples of useful plots to make, and the [seaborn](https://seaborn.pydata.org/) python package is very helpful with this.\n",
        "\n",
        "---\n",
        "> ‚ùó Check out [seaborn's gallery](https://seaborn.pydata.org/examples/index.html) - you can easily see different types of plots possible and make them very simply with your own data!\n",
        "---\n",
        "\n",
        "EDA tools are constantly evolving and improving. While the methods described below are good ways to get started, tools like [D-Tale](https://github.com/man-group/dtale) and [Lux](https://lux-api.readthedocs.io/en/latest/) really make things very easy and fast. Figuring out where NaN, or missing values is also a very important step. Pandas has nice built-in tools, but missingno has some wonderful visualization tools that help point out correlations between where data is missing. D-Tale actually has this built right in now!\n",
        "\n",
        "For more examples of EDA in Python üêç with PyChemAuth, please refer to [this notebook](https://pychemauth.readthedocs.io/en/latest/jupyter/api/eda.html).  \n"
      ],
      "metadata": {
        "id": "6Fa30nGQnp8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Suggestions"
      ],
      "metadata": {
        "id": "7y1LiLn8879N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many EDA approaches so this will not be exhaustive.  If you are doing \"simple\" statistical modeling, checking model assumptions (normality, etc.), looking for outliers, etc. are probably the most important place to begin.  NIST's [engineering statistics handbook](https://www.itl.nist.gov/div898/handbook/eda/eda.htm) is a great reference and a good place to start; specifically, consider the [graphical techniques alphabet](https://www.itl.nist.gov/div898/handbook/eda/section3/eda33.htm) for different enlightening plots to make.\n",
        "\n",
        "The following is just a (growing) list of approaches I have found useful.  \n",
        "\n",
        "* [seaborn](https://seaborn.pydata.org/index.html) has lots of visualization tools; I regularly look at:\n",
        "  * [grouped barplots](https://seaborn.pydata.org/examples/grouped_barplot.html)\n",
        "  * [displots](https://seaborn.pydata.org/examples/faceted_histogram.html) to look at correlations between variables for classification tasks\n",
        "  * [histplots](https://seaborn.pydata.org/examples/kde_ridgeplot.html) or [ridgeplots](https://seaborn.pydata.org/examples/kde_ridgeplot.html) to look at overlapping distributions\n",
        "  * [boxplots](https://seaborn.pydata.org/examples/grouped_boxplot.html)\n",
        "  * [heatmaps](https://seaborn.pydata.org/examples/spreadsheet_heatmap.html) and [clustermaps](https://seaborn.pydata.org/examples/structured_heatmap.html) to visualize correlations\n",
        "\n",
        "* Compute robust statistics to summarize, while minimizing the effect of any outliers you might have:\n",
        "  * [Wikipedia's entry](https://en.wikipedia.org/wiki/Robust_statistics) discusses some examples.\n",
        "  * It can be convenient to use [RobustScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler) instead of the [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) in sklearn workflows.\n",
        "  * Boxplots (listed above) are helpful for identifying outliers by using the quartile ranges to summarize the data.\n",
        "\n",
        "* Compute non-parametric measures:\n",
        "  * Spearman R(ank) order correlation - see [scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html#scipy.stats.spearmanr). sklearn has a nice [tutorial](https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance_multicollinear.html?highlight=spearman) of how to use this to establish multicollinearity between features.  We will cover this in more detail when we go over pre-processing.\n",
        "\n",
        "* Look for class imbalance during classification problems.\n",
        "  * You may need to consider models that can automatically counter-weight to balance this (like trees or SVCs), or use\n",
        "  * SMOTE, or other synthetic data generation approaches. We will also cover this in more detail when we go over pre-processing.\n",
        "  * Use cluster/silhouette analysis to look for \"natural groups\" of data.\n",
        "\n",
        "~~~ python\n",
        "import matplotlib.pyplot as plt\n",
        "plt.bar(\n",
        "    x=y.unique(),\n",
        "    height=[np.sum(y==class) for class in y.unique()]\n",
        ")\n",
        "plt.title('Observations in database')\n",
        "_ = plt.xticks(rotation=45)\n",
        "~~~\n",
        "\n",
        "* Use PCA (unsupervised) in 2-3 dimensions to look for obvious trends on structure ([kernel PCA](https://scikit-learn.org/stable/modules/decomposition.html#kernel-pca) for more non-linear efforts).  Cross validation should be done to check the variability of the principal components (PC) if you decide there is something useful happening and need to explain it.  \n",
        "\n",
        "~~~ python\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "%matplotlib inline # 2D\n",
        "pca = PCA(n_components=2)\n",
        "ss = StandardScaler()\n",
        "X_proj = pca.fit_transform(ss.fit_transform(X))\n",
        "\n",
        "for class_ in y.unique():\n",
        "    mask = y == class_\n",
        "    plt.plot(X_proj[mask,0], X_proj[mask,1], 'o', label=class_)\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel('PC 1')\n",
        "plt.ylabel('PC 2')\n",
        "plt.title('PCA on standardized dataset')\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D # 3D\n",
        "%matplotlib notebook\n",
        "\n",
        "pca = PCA(n_components=3)\n",
        "ss = StandardScaler()\n",
        "X_proj = pca.fit_transform(ss.fit_transform(X))\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "for class_ in y.unique():\n",
        "    mask = y == class_\n",
        "    ax.scatter(X_proj[mask,0], X_proj[mask,1], X_proj[mask,2], label=class_)\n",
        "ax.legend(loc='best')\n",
        "ax.set_xlabel('PC 1')\n",
        "ax.set_ylabel('PC 2')\n",
        "ax.set_zlabel('PC 3')\n",
        "ax.set_title('PCA on standardized dataset')\n",
        "~~~\n",
        "\n",
        "* Use LDA (supervised) for classification tasks to see if there is any natural class division. Take care that you do not over interpret, since this is a supervised technique; cross-validation should always be done to validate any principal components (PCs) you consider relevant.  Looking at LDA coefficients can also be important. See this [stackexchange](https://stackoverflow.com/questions/45692017/lda-vectors-coefficient-interpretation) discussion and [this one](https://stats.stackexchange.com/questions/82497/can-the-scaling-values-in-a-linear-discriminant-analysis-lda-be-used-to-plot-e).\n",
        "\n",
        "~~~ python\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "\n",
        "%matplotlib inline # 2D\n",
        "ss = StandardScaler()\n",
        "lda = LDA(n_components=2, store_covariance=True)\n",
        "X_proj = lda.fit_transform(ss.fit_transform(X), y)\n",
        "\n",
        "for class_ in sorted(y.unique()):\n",
        "    mask = y == class_\n",
        "    plt.plot(X_proj[mask,0], X_proj[mask,1], 'o', label=class_)\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel('PC 1')\n",
        "plt.ylabel('PC 2')\n",
        "plt.title('LDA on standardized dataset')\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D # 3D\n",
        "%matplotlib notebook\n",
        "\n",
        "ss = StandardScaler()\n",
        "lda = LDA(n_components=3, store_covariance=True)\n",
        "X_proj = lda.fit_transform(ss.fit_transform(X), y)\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "for class_ in y.unique():\n",
        "    mask = y == class_\n",
        "    ax.scatter(X_proj[mask,0], X_proj[mask,1], X_proj[mask,2], label=class_)\n",
        "ax.legend(loc='best')\n",
        "ax.set_xlabel('PC 1')\n",
        "ax.set_ylabel('PC 2')\n",
        "ax.set_zlabel('PC 3')\n",
        "ax.set_title('LDA on standardized dataset')\n",
        "~~~\n",
        "\n",
        "* Fit [decision trees](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html?highlight=decision%20tree#sklearn.tree.DecisionTreeClassifier) for classification problems (consider OLS or [multilinear](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html?highlight=linear%20regression#sklearn.linear_model.LinearRegression) for regression tasks); there are [decision tree regressors](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html?highlight=decision%20tree#sklearn.tree.DecisionTreeRegressor) available as well.  If shallow trees work you are in luck - these are very explainable and often useful scientifically.\n",
        "\n",
        "~~~ python\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0, shuffle=True)\n",
        "\n",
        "max_depth = np.arange(1,10)\n",
        "\n",
        "test_acc, train_acc = [], []\n",
        "for md in max_depth:\n",
        "    tree = DecisionTreeClassifier(max_depth=md, class_weight=None) # Can adjust class weighting\n",
        "    # No need to standardize\n",
        "    tree.fit(X_train, y_train)\n",
        "    \n",
        "    test_acc.append(tree.score(X_test, y_test))\n",
        "    train_acc.append(tree.score(X_train, y_train))\n",
        "    \n",
        "plt.plot(max_depth, test_acc, label='Test Accuracy')\n",
        "plt.plot(max_depth, train_acc, label='Train Accuracy')\n",
        "plt.legend(loc='best')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Max depth')\n",
        "~~~\n",
        "\n",
        "* Fit a k-nearest neighbor [classifer](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html?highlight=k%20neighbors#sklearn.neighbors.KNeighborsClassifier) or [regressor](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html?highlight=k%20neighbors#sklearn.neighbors.KNeighborsRegressor) models.\n",
        "  * It can be particularly helpful to do a learning curve, in which you progressively add more and more information to the model, to see if it is converging if it is does/not level out.  If your accuracy does not seem to level out this means your model would continue to improve with more.  Your production model might not be a KNN, but if the KNN flattens out it often indicates that you have enough data to try more advanced learners.  \n",
        "  * If K=1 is found during CV this may also be a sign that your data has sampled the phase space well.\n",
        "\n",
        "~~~ python\n",
        "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0, shuffle=True)\n",
        "\n",
        "ss = StandardScaler() # KNN should use standardized data usually\n",
        "X_train = ss.fit_transform(X_train)\n",
        "X_test = ss.transform(X_test)\n",
        "\n",
        "nebrs = np.arange(1,10)\n",
        "\n",
        "test_acc, train_acc = [], []\n",
        "for n in nebrs:\n",
        "    knn = KNN(n_neighbors=n, p=2, weights='uniform')\n",
        "    knn.fit(X_train, y_train)\n",
        "    \n",
        "    test_acc.append(knn.score(X_test, y_test))\n",
        "    train_acc.append(knn.score(X_train, y_train))\n",
        "    \n",
        "plt.plot(nebrs, test_acc, label='Test Accuracy')\n",
        "plt.plot(nebrs, train_acc, label='Train Accuracy')\n",
        "plt.legend(loc='best')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Number of Neighbors')\n",
        "\n",
        "from ml_inspector.model.InspectModel import learning_curve\n",
        "learning_curve(knn, X=X_train, y=y_train, cv=5)\n",
        "~~~\n",
        "\n",
        "* Fit residuals of any simple models to a Gaussian.  If they fit, it suggests you have extracted all the information and all that is left is random noise.\n",
        "* Plot a confusion matrix for simple models used for classification tasks; this reveals where you might struggle and where you might need to consider doing some feature engineering if different classes get confused with each other.  For example, [polynomial features](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html?highlight=polynomial%20features#sklearn.preprocessing.PolynomialFeatures) can automatically create products of features which can be helpful if there are correlations.  An ROC curve might be an alternative.\n",
        "\n",
        "* Look at the Jensen-Shannon divergence.\n"
      ],
      "metadata": {
        "id": "lMoWFhyT9A-o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Jensen-Shannon Divergence"
      ],
      "metadata": {
        "id": "A0u3jLEn9BA_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the Jensen-Shannon divergence you can develop protocols to:\n",
        "\n",
        "1. Decide if there are features that are particularly helpful at distinguishing a class from the \"rest of the pack\" (one-vs-all). If so, a simple decision trees might be good model.\n",
        "2. If there are plausible macroclasses (merges) that are highly separable using any features; if this is true, there might be a connection between the atomic classes in that macroclass.\n",
        "3. \"Weak\" OvA results can suggest that ensemble methods might succeed where a simple decision tree might fail (boosting).\n",
        "4. \"Weak\" binary JSD values can also suggest that feature engineering might be necessary or worthwhile.\n",
        "\n",
        "See the [PyChemAuth](https://pychemauth.readthedocs.io/en/latest/jupyter/api/feature_selection.html#Jensen-Shannon-Divergence) package for code and details."
      ],
      "metadata": {
        "id": "WhidI5S3-oDk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is the Jensen-Shannon Divergence?"
      ],
      "metadata": {
        "id": "PkBI06T-_MEF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The [Jensen-Shannon divergence](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence) (JSD) is way to describe the difference between two distributions.  It is essentially the mean [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) of a pair of distributions from their mean.\n",
        "\n",
        "$$\n",
        "JSD(P || Q) = \\frac{KLD(P || M) + KLD(Q || M)}{2}\n",
        "$$\n",
        "\n",
        "where $M = \\frac{P+Q}{2}$.  The Kullback-Leibler divergence (KLD) is defined as:\n",
        "\n",
        "$$\n",
        "KLD(P || Q) = \\sum_x P(x) {\\rm log}\\left( \\frac{P(x)}{Q(x)} \\right)\n",
        "$$\n",
        "\n",
        "Importantly, $P$ and $Q$ are **normalized probability distributions**.  Thus, unlike the KL divergence, the JSD is symmetric, and importantly, if a base of 2 is used in the logarithm it is bounded by $0 < JSD < 1$. These are convenient properties as they allow you to quickly estimate how much information you might derive from a feature used in a decision tree, for example."
      ],
      "metadata": {
        "id": "vTuhpmvA-tCM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Developing an Intuition"
      ],
      "metadata": {
        "id": "ak_CR1Qe_PVo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A large JSD between 2 distributions suggests that one or more boundaries can be found to distinguish them.  It may be that one distribution is \"higher\" that another, or that they might interwoven, as illustrated below.  For example, JSD = 1 for either of the instances shown below.\n",
        "\n",
        "<!-- <img style=\"float: center\" src=\"js_example1.png\" width=600px> -->\n",
        "![image](https://user-images.githubusercontent.com/23516749/272694807-57723356-8be3-46f4-8f9e-ed1548c5d418.png)\n",
        "\n",
        "While contrived, it is clear that a decision tree could select distribution A or B (i.e., class A or B), based on cutoffs of $x$.  For the example on the right, a simple stump at $x = 2$ would suffice; for the example on the left, a deeper tree with multiple splits (left and right) would be required, but possible.  Often, the former case (at right above) appears when examining different classes with different characteristic distributions of $x$; the latter (at left above) is more often a symptom of a poorly sampled class (not enough data) where the bins are too fine.  This will be addressed later.\n",
        "\n",
        "These distributions are disjoint and \"maximally different\", so the JSD = 1; but when some overlap occurs, the JSD goes down.  In this example, each distribution has 3 bars where 2 are unique for each.  The resulting JSD = 2/3.\n",
        "\n",
        "<!-- <img style=\"float: center\" src=\"js_example2.png\" width=600px> -->\n",
        "![image](https://user-images.githubusercontent.com/23516749/272694871-dcc4b804-2cc3-4c0b-9217-0984d67285f9.png)\n",
        "\n",
        "When the distributions are identical, the JSD = 0."
      ],
      "metadata": {
        "id": "LM-vkl2e_gnj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### JS Divergence Reveals Plausible Tree Stumps"
      ],
      "metadata": {
        "id": "txIWghIX_kVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As alluded to above, a large JSD between classes for a feature suggests it might be a good feature to use, particularly in simple decision trees.  If the goal is to find a simple model like this, JSD can be a helpful feature selection step, and in doing an initial screen over the data to develop hypotheses (exploratory data analysis).  Note that because class information is used, this is a supervised technique and should only be performed on the training set (or appropriately using pipelines with cross validation).  For example, an sklearn workflow might look like this:\n",
        "\n",
        "~~~ python\n",
        "pipeline = imblearn.pipeline.Pipeline(\n",
        "\t\tsteps=[\n",
        "\t\t\t(\"smote\", ScaledSMOTEENN(random_state=1)),\n",
        "\t\t\t(\"selector\", JensenShannonDivergence(top_k=1,\n",
        "\t\t\t\tper_class=True,\n",
        "\t\t\t\tfeature_names=X.columns)),\n",
        "\t\t\t(\"tree\", DecisionTreeClassifier(random_state=1))\n",
        "\t\t])\n",
        "\n",
        "param_grid = [{\n",
        "\t\t'smote__k_enn':[3, 5, 7, 10],\n",
        "\t\t'smote__k_smote':[3, 5, 7, 10],\n",
        "\t\t'smote__kind_sel_enn':['all', 'mode'],\n",
        "\t\t'tree__max_depth':np.arange(1,4+1),\n",
        "\t\t}]\n",
        "\n",
        "ncv = BiasedNestedCV(k_inner=2, k_outer=5)\n",
        "results = ncv.grid_search(pipeline, param_grid, X.values, y.values)\n",
        "~~~\n",
        "\n",
        "Assume we have some class data with 2 features distributed as follows:\n",
        "\n",
        "<!-- <img style=\"float: center\" src=\"js_example3.png\" width=600px> -->\n",
        "![image](https://user-images.githubusercontent.com/23516749/272695059-e246c90a-2be1-4148-b9ba-8691d3cfdbd3.png)\n",
        "\n",
        "We can compute the JSD by doing a \"one vs. all\" (OvA) comparison; that is, construct the 2 distributions as follows: one from the class of interest, the other from combining all other classes.  After normalizing each, we can compute the JSD to see if the class of interest is sufficiently disjoint along this feature to be considered insightful.  A threshold of 0.7 was chosen, and is shown below.  Clearly, Feature 2 seems to be a good way to distinguish class 2 from the other classes, as expected.\n",
        "\n",
        "<!-- <img style=\"float: center\" src=\"js_example4.png\" width=600px> -->\n",
        "![image](https://user-images.githubusercontent.com/23516749/272695090-f2237fe8-af37-456f-b8c8-bcc8fc4db841.png)\n",
        "\n",
        "With a lower threshold, you might also consider Feature 1 to be helpful for distinguishing class 0 from classes 1 and 2.  In the sklearn workflow given above, the JensenShannonDivergence is used to select the single best dimensions  (top_k=1) for each class; this way you don't have to worry about setting a threshold.  Clearly, this k can also be a hyperparameter that may be optimized also.  See the documentation from the [pychemauth](https://github.com/mahynski/pychemauth) package for details on using this feature selection method as shown.\n",
        "\n",
        "While it might be straightforward to manually visualize such trends when you have a small number of features and classes, with many of them it can be a hard task.  JSD using this OvA approach reduces this to a single floating point number which can be visualized easily with a heatmap, for example.  An example of this will be shown next.\n",
        "\n",
        "**In essence, a high JSD value for a feature with a chosen class means that class \"sticks out\" from the rest of the pack when using that feature.**"
      ],
      "metadata": {
        "id": "2CA8mlCS_oG5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Identifying Clusters"
      ],
      "metadata": {
        "id": "MR29Pesk_3kX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another thing that JSD can be helpful with is identifying when classes are very similar.  Clearly, things with a low JSD have nearly identical distributions which can be a simple and obvious check.  If, for a pair of classes, all their features have a low JSD you might consider these classes as indistinguishable.  That can be interpreted in two ways: either you don't have enough information to tell these classes apart, or you might benefit from considering these classes as a single \"macroclass\";  the latter interpretation suggests a more data-driven approach to determining what \"classes\" can be determined using the data, akin to unsupervised clustering like K-means, etc.  In this case, the use of class information a priori means this is a supervised approach. This means the OvA JSD can be an insightful exploratory data analysis (EDA) tool.  The JSScreen class provides tools to perform the calculations listed below.\n",
        "\n",
        "Consider the following 4 classes with 2 features distributed as follows:\n",
        "\n",
        "<!-- <img style=\"float: center\" src=\"js_example5.png\" width=600px> -->\n",
        "![image](https://user-images.githubusercontent.com/23516749/272695350-77a2d75d-5253-47e9-9428-57e021d81aa8.png)\n",
        "\n",
        "Essentially, A is disjoint from B and C, which are sampled from the same distribution, and D is a broad distribution encompassing all three.\n",
        "\n",
        "When we perform the OvA JSD calculation as described above, we can also perform \"merges\" where we combine classes to create a macroclass (A+B, for example) and see if that macroclass can then be distinguished from the other classes (which is essentially also a macroclass composed of all other classes).  Below is a summary of the JSD computed for both features for all combinations of classes.  Note that the figure is symmetric about its center because the JSD is a symmetric quantity ($JSD(A\\|\\|B) == JSD(B\\|\\|A)$) and since classes are partitioned disjointly (A+B) vs (remainder=C+D) is the same as (C+D) vs. (remainder=A+B).\n",
        "\n",
        "<!-- <img style=\"float: center\" src=\"js_example6.png\" width=600px> -->\n",
        "![image](https://user-images.githubusercontent.com/23516749/272695409-318cf6b9-f567-4bb8-8219-5b3882c67e0e.png)\n",
        "\n",
        "This suggests that A might be separable from classes (B,C,D) based on Feature 1; also that partitioning the classes in (A,B) or (C,D) makes sense.  Since C and D were sampled from the same distribution this is intuitive.  When we consider classes C and D on their own, they get confused with each other.  If you were to look at a confusion matrix of a classifier (say k nearest neighbor, for example) you would see a lot of off-diagonal entries corresponding to confusion between those classes.  Individually, the OvA JSD for classes C and D are less than 0.4 for Feature 1.  If we lump them together as a macroclass (C+D), they do not compete against each other anymore, but instead are compared against the rest of the classes (A and B) and get a OvA JSD of 0.67 >> 0.4.  Because of this symmetry, it falls to the user to decide if the (A,B) set, the (C,D) set, or both are meaningful.  Clearly, because of the overlap, unsupervised clustering would not be able to pick up the connection between C and D.  \n",
        "\n",
        "As already suggested, a KNN classifier's confusion matrix can provide a similar insight into class similarities.  A KNN model relies on some distance function (Euclidean vs. Mahattan, for example) which is not unique, though; moreover, this distance is computed using all the features at once, whereas here we can inspect each feature's similarity individually.\n",
        "\n",
        "Assuming we had many features and wished to find a subset of useful features to focus on, we might consider the maximum OvA JS divergence across all features for all (macro)classes; this gives a sense of how much we could hope to separate each (macro)class from the rest of the classes.  Because we may have many features which are not very useful, the max() is often more meaningful than the mean() of these for all the features.  A column-wise max() operation, sorted by value, looks like this (the errors are the standard deviation, which is not very meaningful for this example):\n",
        "\n",
        "<!-- <img style=\"float: center\" src=\"js_example7.png\" width=600px> -->\n",
        "![image](https://user-images.githubusercontent.com/23516749/272695460-b33298fb-77d0-4a52-b0a2-34f8e398ea97.png)\n",
        "\n",
        "Clearly, we see the (A) and (B,C) sets being indicated as potentially easily separable based on a single feature.  Their complements also appear with the same JSD.  Again, it is up to you to decide if A as a class is meaningful  or if (B,C,D) as a macroclass does.  Below are the binary distributions used for comparison for the top 4 (macro)classes based on the feature with the largest JSD (in this case it is always Feature 1); remember, these are normalized before computing the JSD.\n",
        "\n",
        "<!-- <img style=\"float: center\" src=\"js_example8.png\" width=600px> -->\n",
        "![image](https://user-images.githubusercontent.com/23516749/272695501-e87c4c00-8454-461e-8478-75d86b2ab062.png)\n"
      ],
      "metadata": {
        "id": "ckiYmcNj_53H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Binary vs OvA"
      ],
      "metadata": {
        "id": "YvQw4wXeAK95"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is also possible to perform a binary comparison directly between classes.  This disregards the information from the rest of the dataset (classes) and just because A and B are distinguishable from each other in isolation, they may not be when other classes are present since they may be overlapped with those.  Still, it can be helpful to see how different classes really are in terms of their feature distributions.  Below, at left, is the binary JSD for the classes we just looked at.  Clearly B and C are essentially identical, and D is not easily distinguished from any of them.  It can sometimes be helpful to perform feature engineering to amplify hidden correlations that can create differences.  Below, at right, polynomial features to [order 3](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) were created and re-evaluated; this creates products of columns which can be helpful if correlations exist.  In this case, the improvements to B and C are minimal because the distributions were purposefully sampled from the same distribution.\n",
        "\n",
        "<!-- <img style=\"float: center\" src=\"js_example13.png\" width=600px> -->\n",
        "![image](https://user-images.githubusercontent.com/23516749/272695577-2d224575-a331-42eb-8448-2716248c1adc.png)\n",
        "\n",
        "As a counterexample, consider the binary system with 2 features depicted below (left).  The distributions over features 1 and 2 are similar, but there is a correlation (product) that was used to generate the classes for this example.  When polynomial features are added, indeed, the product feature is clearly useful for separating the classes (right).\n",
        "\n",
        "<!-- <img style=\"float: center\" src=\"js_example14.png\" width=300px>\n",
        "<img style=\"float: center\" src=\"js_example15.png\" width=300px> -->\n",
        "![image](https://user-images.githubusercontent.com/23516749/272695655-cbf6b54f-2ef1-4eb0-9f20-3f832813a202.png)\n",
        "![image](https://user-images.githubusercontent.com/23516749/272695677-0b8c3cbe-924c-4105-aa11-15126e7c15c0.png)\n",
        "\n",
        "The binary JSD clearly reflect that in the original case (no polynomical feature engineering) the maximum divergence was very low, but afterwards is quite high.  Indeed, the \"product\" feature is the one that leads to JSD given below (0.97).\n",
        "\n",
        "<!-- <img style=\"float: center\" src=\"js_example16.png\" width=600px> -->\n",
        "![image](https://user-images.githubusercontent.com/23516749/272695730-0a493f9c-4668-4666-97fd-58a5c9a514de.png)"
      ],
      "metadata": {
        "id": "7IuEimf_ALFZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### When to Consider Ensemble Methods"
      ],
      "metadata": {
        "id": "HmaRRZddAPj5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Binary JSD only imply that open class is highly differentiable from another based on simple thresholds.  If you see a lot of large binary JSD values in a row or column above this suggests that this class can be distinguished from all other classes easily somehow, though it may be a different feature used for each comparison. In this case, deep trees to make a series of comparisons might be necessary; however, these do not generalize well as they tend to overfit strongly based on the training data so you need a lot of \"stable\" data to make these work.  Random forests might be a good alternative to using deep trees in this case. It may also happen that there is no single strong OvA result, but perhaps a \"column\" (features) of weak (JSD $\\sim$ 0.5) ones instead; this might also suggest that an ensemble approach could succeed where a simpler model is likely to fail. Specifically, a gradient boosted tree might a good choice because it make use of many stumps (weak learners) in series to produce a strong model."
      ],
      "metadata": {
        "id": "H210cdqyAPmY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Common Pitfalls"
      ],
      "metadata": {
        "id": "StoH3o8xAUxO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are a few pitfalls to this approach. They essentially boil down to how to discretize the distributions to compute the KL divergence in the first place.  In practice, the distributions for a given feature are histogrammed into discrete bins, from which the KLD and JSD can be easily computed.  As a side note, in practice some small amount added to all bins (~1.0e-12) since numerically, we cannot divide by zero in the KLD calculation.  This is a small affect usually, should be noted."
      ],
      "metadata": {
        "id": "Np12X0e8AUzC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bins are too coarse"
      ],
      "metadata": {
        "id": "L8SzT7n0AYfG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bins that are too coarse will make distributions seem identical.  For example, selecting $n_{\\rm bins}=1$ means a single bin, which clearly will then encompass everything leading to JSD = 0 between any two distributions.  This is a silly example, and in practice, this limit arises more due to the presence of outliers.  Typically, we select the bin width after selecting $n_{\\rm bins}$ to span the min and max of the observations; as a result, outliers will coarsen the bins.  In this example, classes A and B from above are compared before (left) and after (right) some outliers are added to A.  The divergences are, again symmetric, but are quite high (above the 0.7 threshold chosen earlier) for the case without the outlier; this is expected since A and B were purposefully prepared to be different.  However, when the outliers are added to A, the overlap between A and B becomes amplified because of bin coarsening, dramatically reducing this divergence which might make you miss the fact that Feature 1 is, in fact, a good feature to use.\n",
        "\n",
        "<!-- <img style=\"float: center\" src=\"js_example9.png\" width=300px>\n",
        "<img style=\"float: center\" src=\"js_example10.png\" width=300px> -->\n",
        "![image](https://user-images.githubusercontent.com/23516749/272695880-6c40a55c-01f3-4050-ac4a-528d53405435.png)\n",
        "![image](https://user-images.githubusercontent.com/23516749/272695899-5d7e5a03-6b19-42ec-8322-ee655adae95a.png)"
      ],
      "metadata": {
        "id": "nGgbjGX8AYg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bins are too fine"
      ],
      "metadata": {
        "id": "QF42_uxmAcys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we choose bin widths that are too small, each individual measurement (floating point number) becomes isolated to some arbitrarily small neighborhood and overlapping distributions begin to look as though they do not overlap at all.  This depends on the amount of data you have; more samples means you can tolerate smaller bins, but again, this is subjective and difficult to detemine in advance.  In the example below, we compute the OvA JSD calculations using $n_{\\rm bins}=500$ (left) vs. $n_{\\rm bins}=25$ for the B and C distributions above (designed to be overlapping).  When the number of bins is too high (small bin size) the divergence appears large (>0.7); whereas when a reasonable number is chosen the JSD is small (correct) because the two distributions are highly overlapping.\n",
        "\n",
        "<!-- <img style=\"float: center\" src=\"js_example11.png\" width=300px>\n",
        "<img style=\"float: center\" src=\"js_example12.png\" width=300px> -->\n",
        "![image](https://user-images.githubusercontent.com/23516749/272695931-5d56c5f2-dacf-4762-9c33-9a6e75bd356d.png)\n",
        "![image](https://user-images.githubusercontent.com/23516749/272695954-aa1a087b-7a66-4490-a963-b74428b7a296.png)"
      ],
      "metadata": {
        "id": "eMRmboPZAc09"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### In practice"
      ],
      "metadata": {
        "id": "IGDNhwtIAgx3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In practice, having a bin width approach zero leads to $JSD \\rightarrow 1$ while a bin width that becomes infinitely large cases $JSD \\rightarrow 0$, in all causes for all distributions.  In practice, it is very difficult to assess what an appropriate bin width is then.  The easier approach in practice is to choose $n_{\\rm bins}$; the bin width will be set by the bounds of the distributions and it is much easier to use existing techniques to remove potential outliers which would skew the results.  Even if the outliers are removed, choosing a bad bin width could still cause the JSD to jump toward unity.  Simply choosing a reasonable $n_{\\rm bins} \\sim 25$ is usually easier.  One way to remove outliers is to remove all points outside the IQR and use only this subset for the JSD-based comparisons.  Other approaches could also be devised.\n",
        "\n",
        "The number of bins could be considered an adjustable hyperparameter if using the sklearn workflow, or something similar, shown above.  However, this is not necessaily a reasonable thing to do; it is better to choose some reasonable bin width (via the number of bins) and simply select the top $k$ features, treating $k$ as the hyperparameter; alternatively, you could set the threshold for deciding a feature is useful since the value used here (0.7) is arbitrary."
      ],
      "metadata": {
        "id": "KtiQ0EvBAg1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipelines"
      ],
      "metadata": {
        "id": "JpYyOZOddKS2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://pychemauth.readthedocs.io/en/latest/_images/pipeline.png\" height=400 align=\"right\"/>\n",
        "\n",
        "In scikit-learn, [pipelines](https://scikit-learn.org/stable/modules/compose.html) üî© are composite estimators.  They are composed of a series of steps, illustrated at the right.  Each step in the pipeline is a [\"transformer\"](https://scikit-learn.org/stable/glossary.html#term-transformer) which is a class that implements `fit` and `transform` methods.  \n",
        "\n",
        "* üöÜ During training, both are called in sequence, where the former \"learns\" any parameters needed (such as the mean or standard deviation of the data it sees).  \n",
        "* ü•º During testing, these parameters are fixed and simply used to transform the data.  \n",
        "\n",
        "Each transformer is responsible for accepting the data from the last step, transforming it, then handing it to the next step.  This continues in sequence until we reach the last step.  The last step can be a transformer too, but for our purposes we put a predictive model there (either regression or classification model).  The model should implement `fit` and `predict` methods.  The figure at the right shows how data flows through the pipeline at different stages.\n",
        "\n",
        "üëâ When a pipeline is `fit` it sends data through the blue path on the right, going through the `fit` members of all steps; at the final step `predict` is called.  The pipeline is scored using the `score` member of the model.\n",
        "\n",
        "We will refer to all the steps leading up to to model (last step) as the \"pre-processing\" steps.  Pre-processing will be covered in detail later, but this is the part of the pipeline it goes with.  There are also many different models, which will also be covered in detail later on.  However, before we get there, it is important to get a üê¶ \"bird's eye view\" of the main structure we will be working with.\n",
        "\n",
        "‚ùì Q: Why is this the main structure we use for modeling?\n",
        "\n",
        "üôã A: Because this creates an end-to-end process that can accept the data, do all the cleaning and processing, model the data, and make a prediction in one structure.  That makes it simple to optimize and reproduce!\n",
        "\n",
        "Because sklearn's estimator API is so widespread, when new models or preprocessing steps are developed by the ML or related communities, they are usually published in a compatible format.  This means that when new developments occur, we can simply \"drop them in\" the appropriate slot in the pipeline above and try them out!  This enables:\n",
        "* side-by-side comparison of different approaches\n",
        "* development of best practices\n",
        "* \"future-proof\"(*) your work\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3TMP_8tOdKU0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For examples of using pipelines in Python üêç with PyChemAuth, please refer to [this notebook](https://pychemauth.readthedocs.io/en/latest/jupyter/api/pipelines.html)."
      ],
      "metadata": {
        "id": "HuNW9WTzCDnt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation metrics"
      ],
      "metadata": {
        "id": "fBVCjtpJdKW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By [default](https://scikit-learn.org/stable/modules/model_evaluation.html) a pipeline uses the `score` function of the last step in the pipeline.  Recall our simple example:\n",
        "\n",
        "```python\n",
        "class MyClassifier(BaseEstimator, ClassifierMixin):\n",
        "    \"\"\"\n",
        "    A simple nearest neighbor classifier.\n",
        "    \"\"\"\n",
        "    def __init__(self, demo_param='demo'):\n",
        "        self.demo_param = demo_param\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Check that X and y have correct shape\n",
        "        X, y = check_X_y(X, y)\n",
        "\n",
        "        # Store the classes seen during fit\n",
        "        self.classes_ = unique_labels(y)\n",
        "        self.X_ = X\n",
        "        self.y_ = y\n",
        "\n",
        "        # Return the classifier\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Check if fit has been called\n",
        "        check_is_fitted(self)\n",
        "\n",
        "        # Input validation\n",
        "        X = check_array(X)\n",
        "\n",
        "        closest = np.argmin(euclidean_distances(X, self.X_), axis=1)\n",
        "\n",
        "        return self.y_[closest]\n",
        "\n",
        "    def score(self, X, y):\n",
        "        check_is_fitted(self)\n",
        "        X, y = check_X_y(X, y)\n",
        "\n",
        "        predictions = self.predict(X)\n",
        "        accuracy = np.sum(predictions == y) / len(y)\n",
        "\n",
        "        return accuracy\n",
        "```        \n",
        "\n",
        "sklearn allows you to [define custom scoring functions](https://scikit-learn.org/stable/modules/model_evaluation.html), or make use of many built-in alternatives. You will not need to specify the `score` function for any examples in this course, or for models implemented in PyChemAuth; however, if you wish to see how a model (or pipeline which terminates with a given model) is being scored you can investigate the `score` member of the model used.\n",
        "\n",
        "```\n",
        ">>> ?model.score\n",
        "```"
      ],
      "metadata": {
        "id": "jTdKVtE1dKZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross-Validation"
      ],
      "metadata": {
        "id": "EbqSl9zCdSAK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://scikit-learn.org/stable/_images/grid_search_workflow.png\" align=\"right\" height=300 />\n",
        "\n",
        "[Cross-validation (CV)](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) is used to estimate the generalization error of a model, that is, how well it performs on unseen data. To be accurate, we must avoid testing on data used during model training since the model will be biased to fit that data. Any estimate of model performance using training data will be overly optimistic; however, it is not always clear by how much, and the scarcity of data often drives one to make assumptions that allow this data to be re-used. Various methods of CV exist to balance the amount of effort expended and assumptions made to obtain this estimate.\n",
        "\n",
        "sklearn has extensive [documentation](https://scikit-learn.org/stable/modules/cross_validation.html) of the different types of cross-validation tools available in the package; however, we will mostly use \"stratified, k-fold\" CV.  In this type of cross-validation:\n",
        "* \"k-fold\" means the data is split into $k$ equal-sized segments (usually after shuffling); training is repeated $k$ times with a different single segment held out as a test set, while the remainder are used for training.\n",
        "* \"stratified\" means that the proportion of different classes is kept as constant as possible across the different splits.  For example, if the data is 80% A and 20% B, each split will also be about this ratio.  This is important so you do not end up with strongly biased folds."
      ],
      "metadata": {
        "id": "W87so6r3y9xY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For examples of cross-validating pipelines in Python üêç with PyChemAuth, please refer to [this notebook](https://pychemauth.readthedocs.io/en/latest/jupyter/learn/cv_optimization.html)."
      ],
      "metadata": {
        "id": "7inOhZGezKYt"
      }
    }
  ]
}